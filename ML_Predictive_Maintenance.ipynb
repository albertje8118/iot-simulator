{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64e3f9c",
   "metadata": {},
   "source": [
    "# Microsoft Fabric ML Predictive Maintenance\n",
    "## Predict Component Replacement Using Machine Learning\n",
    "\n",
    "**Objectives:**\n",
    "1. Upload historical CSV data to Microsoft Fabric Lakehouse\n",
    "2. Create features for machine learning\n",
    "3. Train predictive model using flaml.AutoML\n",
    "4. Generate predictions and save to Lakehouse\n",
    "5. Visualize ML predictions in Power BI\n",
    "\n",
    "**Prerequisites:**\n",
    "- Microsoft Fabric workspace with Lakehouse named `MaintenanceML`\n",
    "- CSV file: `quality_30days.csv` uploaded to Lakehouse Files\n",
    "- This notebook attached to `MaintenanceML` Lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1e0a4",
   "metadata": {},
   "source": [
    "## Part 1: Upload Data to Fabric Lakehouse\n",
    "\n",
    "**Manual Steps (before running this notebook):**\n",
    "\n",
    "1. Go to Microsoft Fabric portal: https://app.fabric.microsoft.com\n",
    "2. Navigate to your workspace ‚Üí **+ New** ‚Üí **Lakehouse**\n",
    "3. Name: `MaintenanceML` ‚Üí **Create**\n",
    "4. Click **Get data** ‚Üí **Upload files**\n",
    "5. Select `quality_30days.csv` ‚Üí Wait for upload\n",
    "6. Attach this notebook to the Lakehouse: **Add** ‚Üí select `MaintenanceML`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111cd48",
   "metadata": {},
   "source": [
    "## Part 1.1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806dd402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbbccb3",
   "metadata": {},
   "source": [
    "## Part 1.2: Load CSV to Lakehouse Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV from Lakehouse Files into a Spark DataFrame\n",
    "csv_path = \"Files/quality_30days.csv\"\n",
    "df_spark = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_path)\n",
    "\n",
    "# Save to Lakehouse table\n",
    "df_spark.write.mode(\"overwrite\").saveAsTable(\"MaintenanceML.machine_data_raw\")\n",
    "\n",
    "print(f\"‚úÖ CSV loaded to table: MaintenanceML.machine_data_raw\")\n",
    "print(f\"Total rows: {df_spark.count():,}\")\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22825299",
   "metadata": {},
   "source": [
    "## Part 2: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data from Lakehouse table\n",
    "df = spark.read.table(\"MaintenanceML.machine_data_raw\").toPandas()\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df = df.sort_values(['MachineID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded {len(df):,} records\")\n",
    "print(f\"Date range: {df['Timestamp'].min()} to {df['Timestamp'].max()}\")\n",
    "print(f\"Machines: {df['MachineID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad5425",
   "metadata": {},
   "source": [
    "## Part 3: Calculate Rotation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc08e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rotation count per cycle\n",
    "df['RotationCount'] = df['ActualAngle'] / 360.0\n",
    "\n",
    "# Cumulative rotations per machine (key wear indicator)\n",
    "df['CumulativeRotation'] = df.groupby('MachineID')['RotationCount'].cumsum()\n",
    "\n",
    "# Cumulative bit rotations (use BitRotationCounter if available)\n",
    "if 'BitRotationCounter' in df.columns:\n",
    "    df['CumulativeBitRotation'] = df['BitRotationCounter']\n",
    "else:\n",
    "    df['CumulativeBitRotation'] = df['CumulativeRotation']\n",
    "\n",
    "print(\"‚úÖ Rotation features created\")\n",
    "df[['Timestamp', 'MachineID', 'RotationCount', 'CumulativeBitRotation']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd14c77",
   "metadata": {},
   "source": [
    "## Part 4: Create Rolling Window Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set timestamp as index for rolling calculations\n",
    "df_indexed = df.set_index('Timestamp')\n",
    "\n",
    "# Create rolling features (1 hour window)\n",
    "rolling_features = df_indexed.groupby('MachineID').rolling('1H').agg({\n",
    "    'RotationCount': ['sum', 'mean'],\n",
    "    'ActualTorque': ['mean', 'std'],\n",
    "    'CycleTime_ms': ['mean', 'max'],\n",
    "    'CycleOK': 'mean'  # Pass rate in last hour\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "rolling_features.columns = ['MachineID', 'Timestamp', \n",
    "                             'Rot_LastHour_Sum', 'Rot_LastHour_Avg',\n",
    "                             'Torque_LastHour_Avg', 'Torque_LastHour_Std',\n",
    "                             'CycleTime_LastHour_Avg', 'CycleTime_LastHour_Max',\n",
    "                             'PassRate_LastHour']\n",
    "\n",
    "# Merge back to main dataframe\n",
    "df = df.merge(rolling_features, on=['MachineID', 'Timestamp'], how='left')\n",
    "\n",
    "# Fill NaN with 0 for first hour\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(\"‚úÖ Rolling window features created\")\n",
    "df[['MachineID', 'Rot_LastHour_Sum', 'Torque_LastHour_Avg', 'PassRate_LastHour']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c3889",
   "metadata": {},
   "source": [
    "## Part 5: Create Target Variable (Remaining Rotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc6c78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define component lifetime (from reference data)\n",
    "BIT_LIFETIME = 100000  # rotations\n",
    "\n",
    "# Calculate remaining rotations until replacement\n",
    "df['RemainingRotations'] = BIT_LIFETIME - df['CumulativeBitRotation']\n",
    "\n",
    "# Calculate days until replacement (assuming current rotation rate)\n",
    "df['RotationsPerHour'] = df['Rot_LastHour_Sum']\n",
    "df['HoursUntilReplacement'] = df['RemainingRotations'] / (df['RotationsPerHour'] + 0.1)  # avoid divide by zero\n",
    "df['DaysUntilReplacement'] = df['HoursUntilReplacement'] / 24\n",
    "\n",
    "# Clip predictions to reasonable range (0-365 days)\n",
    "df['DaysUntilReplacement'] = df['DaysUntilReplacement'].clip(0, 365)\n",
    "\n",
    "print(\"‚úÖ Target variable created\")\n",
    "print(f\"Average days until replacement: {df['DaysUntilReplacement'].mean():.1f}\")\n",
    "df[['MachineID', 'CumulativeBitRotation', 'RemainingRotations', 'DaysUntilReplacement']].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c29bd",
   "metadata": {},
   "source": [
    "## Part 6: Save Feature Table to Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc056e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for ML model\n",
    "feature_columns = [\n",
    "    'Timestamp', 'MachineID', 'ProductID', 'ScrewPosition',\n",
    "    'CumulativeBitRotation', 'RotationCount',\n",
    "    'Rot_LastHour_Sum', 'Rot_LastHour_Avg',\n",
    "    'Torque_LastHour_Avg', 'Torque_LastHour_Std',\n",
    "    'CycleTime_LastHour_Avg', 'CycleTime_LastHour_Max',\n",
    "    'PassRate_LastHour',\n",
    "    'ActualTorque', 'ActualAngle', 'CycleTime_ms',\n",
    "    'DaysUntilReplacement'  # Target variable\n",
    "]\n",
    "\n",
    "df_features = df[feature_columns].copy()\n",
    "\n",
    "# Convert back to Spark DataFrame and save to Lakehouse\n",
    "spark_df = spark.createDataFrame(df_features)\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"MaintenanceML.ml_features\")\n",
    "\n",
    "print(\"‚úÖ Feature table saved to Lakehouse: ml_features\")\n",
    "print(f\"Total rows: {len(df_features):,}\")\n",
    "print(f\"Feature columns: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d076fc0",
   "metadata": {},
   "source": [
    "## Part 7: Train ML Model with flaml.AutoML\n",
    "\n",
    "**Using Microsoft Fabric AutoML (flaml library)**\n",
    "\n",
    "According to Microsoft Fabric documentation, use `flaml.AutoML` for code-first automated machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a28763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ML Model using flaml.AutoML (Fabric-native AutoML)\n",
    "import mlflow\n",
    "from flaml import AutoML\n",
    "\n",
    "# Set up MLflow experiment\n",
    "EXPERIMENT_NAME = \"Predictive_Maintenance_Experiment\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Prepare training data\n",
    "feature_cols = [\n",
    "    'CumulativeBitRotation', 'RotationCount',\n",
    "    'Rot_LastHour_Sum', 'Rot_LastHour_Avg',\n",
    "    'Torque_LastHour_Avg', 'Torque_LastHour_Std',\n",
    "    'CycleTime_LastHour_Avg', 'CycleTime_LastHour_Max',\n",
    "    'PassRate_LastHour',\n",
    "    'ActualTorque', 'ActualAngle', 'CycleTime_ms'\n",
    "]\n",
    "\n",
    "X_train = df_features[feature_cols]\n",
    "y_train = df_features['DaysUntilReplacement']\n",
    "\n",
    "# Initialize AutoML\n",
    "automl = AutoML()\n",
    "\n",
    "# Configure AutoML settings\n",
    "settings = {\n",
    "    \"time_budget\": 300,  # 5 minutes training time\n",
    "    \"metric\": \"r2\",  # R-squared for regression\n",
    "    \"task\": \"regression\",\n",
    "    \"log_file_name\": \"automl_maintenance.log\",\n",
    "    \"seed\": 12345\n",
    "}\n",
    "\n",
    "# Start AutoML training with MLflow logging\n",
    "with mlflow.start_run(run_name=\"AutoML_Predictive_Maintenance\") as run:\n",
    "    automl.fit(X_train=X_train, y_train=y_train, **settings)\n",
    "    \n",
    "    # Get the best model (the actual estimator, not the AutoML wrapper)\n",
    "    best_model = automl.model\n",
    "    \n",
    "    # Log best model metrics\n",
    "    mlflow.log_metric(\"best_r2\", automl.best_loss)\n",
    "    mlflow.log_metric(\"best_estimator_r2\", 1 - automl.best_loss)\n",
    "    mlflow.log_param(\"best_estimator\", str(automl.best_estimator))\n",
    "    mlflow.log_param(\"best_config\", str(automl.best_config))\n",
    "    \n",
    "    # Log the actual trained model (not the AutoML wrapper)\n",
    "    mlflow.sklearn.log_model(best_model, \"model\")\n",
    "    \n",
    "    print(f\"‚úÖ Best estimator: {automl.best_estimator}\")\n",
    "    print(f\"‚úÖ Best loss: {automl.best_loss:.4f}\")\n",
    "    print(f\"‚úÖ Best R¬≤: {1 - automl.best_loss:.4f}\")\n",
    "    print(f\"‚úÖ Model saved to MLflow experiment: {EXPERIMENT_NAME}\")\n",
    "    print(f\"‚úÖ Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb88faf",
   "metadata": {},
   "source": [
    "## Part 8: Load Trained Model from MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ca33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model from MLflow\n",
    "loaded_model = mlflow.sklearn.load_model(f\"runs:/{run.info.run_id}/model\")\n",
    "\n",
    "print(f\"‚úÖ Model loaded successfully from run: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a1d9b",
   "metadata": {},
   "source": [
    "## Part 9: Prepare Latest Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature table\n",
    "df_features_full = spark.read.table(\"MaintenanceML.ml_features\").toPandas()\n",
    "\n",
    "# Get latest state per machine (most recent timestamp)\n",
    "df_latest = df_features_full.sort_values('Timestamp').groupby('MachineID').tail(1).reset_index(drop=True)\n",
    "\n",
    "# Select feature columns (exclude target and identifiers)\n",
    "X_predict = df_latest[feature_cols]\n",
    "\n",
    "print(f\"Predicting for {len(df_latest)} machines\")\n",
    "df_latest[['MachineID', 'Timestamp', 'CumulativeBitRotation']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4ad97",
   "metadata": {},
   "source": [
    "## Part 10: Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc422c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = loaded_model.predict(X_predict)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_latest['ML_PredictedDays'] = predictions\n",
    "\n",
    "# Calculate confidence/risk level\n",
    "df_latest['RiskLevel'] = pd.cut(\n",
    "    df_latest['ML_PredictedDays'],\n",
    "    bins=[0, 2, 7, 14, 365],\n",
    "    labels=['üî¥ CRITICAL', 'üü† URGENT', 'üü° WARNING', 'üü¢ GOOD']\n",
    ")\n",
    "\n",
    "# Add prediction timestamp\n",
    "df_latest['PredictionTimestamp'] = datetime.utcnow()\n",
    "\n",
    "print(\"‚úÖ Predictions generated\")\n",
    "df_latest[['MachineID', 'ML_PredictedDays', 'RiskLevel', 'CumulativeBitRotation']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3a828",
   "metadata": {},
   "source": [
    "## Part 10.1: Load New CSV Data for Prediction (Optional)\n",
    "\n",
    "**Use this section to predict on new CSV data from IoT simulator**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8849c84e",
   "metadata": {},
   "source": [
    "## Part 11: Save Predictions to Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6ea9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for prediction table\n",
    "prediction_cols = [\n",
    "    'PredictionTimestamp', 'Timestamp', 'MachineID',\n",
    "    'CumulativeBitRotation', 'RotationCount',\n",
    "    'Rot_LastHour_Sum', 'PassRate_LastHour',\n",
    "    'DaysUntilReplacement',  # Rule-based prediction\n",
    "    'ML_PredictedDays',       # ML prediction\n",
    "    'RiskLevel'\n",
    "]\n",
    "\n",
    "df_predictions = df_latest[prediction_cols].copy()\n",
    "\n",
    "# Convert to Spark and save\n",
    "spark_predictions = spark.createDataFrame(df_predictions)\n",
    "spark_predictions.write.mode(\"overwrite\").saveAsTable(\"MaintenanceML.ml_predictions\")\n",
    "\n",
    "print(\"‚úÖ Predictions saved to: MaintenanceML.ml_predictions\")\n",
    "print(f\"Total predictions: {len(df_predictions)}\")\n",
    "print(\"\\nüìä Risk Distribution:\")\n",
    "print(df_predictions['RiskLevel'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bfc1de",
   "metadata": {},
   "source": [
    "## Part 12: Connect Power BI to Lakehouse (Instructions)\n",
    "\n",
    "**Manual Steps in Power BI Desktop:**\n",
    "\n",
    "1. Open **Power BI Desktop**\n",
    "2. Click **Home** ‚Üí **Get Data** ‚Üí **More**\n",
    "3. Search for: **Microsoft Fabric**\n",
    "4. Select **Lakehouse** ‚Üí **Connect**\n",
    "5. Sign in with your Fabric credentials\n",
    "6. Navigate to workspace ‚Üí Select `MaintenanceML` lakehouse\n",
    "7. Select tables:\n",
    "   - ‚úÖ `ml_predictions`\n",
    "   - ‚úÖ `machine_data_raw` (optional)\n",
    "8. Click **Load**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99159ddc",
   "metadata": {},
   "source": [
    "## Part 13: DAX Measures for Power BI (Reference)\n",
    "\n",
    "**Create these measures in Power BI:**\n",
    "\n",
    "```dax\n",
    "// Average ML Prediction\n",
    "AvgML_PredictedDays = AVERAGE(ml_predictions[ML_PredictedDays])\n",
    "\n",
    "// Average Rule-Based Prediction\n",
    "AvgRule_PredictedDays = AVERAGE(ml_predictions[DaysUntilReplacement])\n",
    "\n",
    "// Prediction Difference\n",
    "PredictionDifference = [AvgML_PredictedDays] - [AvgRule_PredictedDays]\n",
    "\n",
    "// Critical Machines Count\n",
    "CriticalMachines = \n",
    "CALCULATE(\n",
    "    DISTINCTCOUNT(ml_predictions[MachineID]),\n",
    "    ml_predictions[RiskLevel] = \"üî¥ CRITICAL\"\n",
    ")\n",
    "```\n",
    "\n",
    "**Dashboard Visuals:**\n",
    "1. KPI Cards: Avg ML Days, Avg Rule Days, Prediction Difference, Critical Machines\n",
    "2. Matrix: Machine Risk Status (rows: MachineID, columns: RiskLevel)\n",
    "3. Bar Chart: ML vs Rule-Based by Machine\n",
    "4. Scatter Plot: Rotation vs Predicted Days\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6863acfb",
   "metadata": {},
   "source": [
    "## Part 14: Schedule Automated Predictions (Optional)\n",
    "\n",
    "**Create Data Pipeline in Microsoft Fabric:**\n",
    "\n",
    "1. In Fabric workspace: **+ New** ‚Üí **Data pipeline**\n",
    "2. Name: `Daily_ML_Predictions`\n",
    "3. Add activities:\n",
    "   - **Notebook** ‚Üí Select this notebook\n",
    "   - Configure: Run all cells\n",
    "4. Set schedule:\n",
    "   - **Schedule** tab\n",
    "   - Frequency: Daily\n",
    "   - Time: 1:00 AM UTC\n",
    "5. Click **Publish**\n",
    "\n",
    "**Alternative: Use this code to run specific sections**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35f682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated daily prediction workflow (run this cell for scheduled updates)\n",
    "\n",
    "# 1. Reload latest data from Lakehouse\n",
    "df_latest_update = spark.read.table(\"MaintenanceML.machine_data_raw\").toPandas()\n",
    "\n",
    "# 2. Apply same feature engineering (reuse code from Parts 2-5)\n",
    "# ... (feature engineering code)\n",
    "\n",
    "# 3. Load model and generate predictions\n",
    "predictions_update = loaded_model.predict(X_predict)\n",
    "\n",
    "# 4. Save updated predictions\n",
    "# ... (save to Lakehouse)\n",
    "\n",
    "print(\"‚úÖ Daily prediction update completed\")\n",
    "print(f\"Timestamp: {datetime.utcnow()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new CSV data (e.g., sample_quality_data.csv from IoT simulator)\n",
    "csv_file = \"Files/sample_quality_data.csv\"  # Upload this file to Lakehouse Files first\n",
    "\n",
    "# Read CSV directly to pandas\n",
    "df_new = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(csv_file) \\\n",
    "    .toPandas()\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df_new['Timestamp'] = pd.to_datetime(df_new['Timestamp'])\n",
    "df_new = df_new.sort_values(['MachineID', 'Timestamp']).reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_new):,} new records\")\n",
    "print(f\"Date range: {df_new['Timestamp'].min()} to {df_new['Timestamp'].max()}\")\n",
    "print(f\"Machines: {df_new['MachineID'].nunique()}\")\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4ba41",
   "metadata": {},
   "source": [
    "## Part 10.2: Feature Engineering on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same feature engineering as training data\n",
    "\n",
    "# Step 1: Calculate rotation count per cycle\n",
    "df_new['RotationCount'] = df_new['ActualAngle'] / 360.0\n",
    "\n",
    "# Step 2: Cumulative rotations per machine\n",
    "df_new['CumulativeRotation'] = df_new.groupby('MachineID')['RotationCount'].cumsum()\n",
    "\n",
    "# Step 3: Use BitRotationCounter if available\n",
    "if 'BitRotationCounter' in df_new.columns:\n",
    "    df_new['CumulativeBitRotation'] = df_new['BitRotationCounter']\n",
    "else:\n",
    "    df_new['CumulativeBitRotation'] = df_new['CumulativeRotation']\n",
    "\n",
    "# Step 4: Rolling window features (1 hour)\n",
    "df_new_indexed = df_new.set_index('Timestamp')\n",
    "rolling_features_new = df_new_indexed.groupby('MachineID').rolling('1H').agg({\n",
    "    'RotationCount': ['sum', 'mean'],\n",
    "    'ActualTorque': ['mean', 'std'],\n",
    "    'CycleTime_ms': ['mean', 'max'],\n",
    "    'CycleOK': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "rolling_features_new.columns = ['MachineID', 'Timestamp', \n",
    "                                 'Rot_LastHour_Sum', 'Rot_LastHour_Avg',\n",
    "                                 'Torque_LastHour_Avg', 'Torque_LastHour_Std',\n",
    "                                 'CycleTime_LastHour_Avg', 'CycleTime_LastHour_Max',\n",
    "                                 'PassRate_LastHour']\n",
    "\n",
    "df_new = df_new.merge(rolling_features_new, on=['MachineID', 'Timestamp'], how='left')\n",
    "df_new = df_new.fillna(0)\n",
    "\n",
    "# Step 5: Get latest state per machine\n",
    "df_new_latest = df_new.sort_values('Timestamp').groupby('MachineID').tail(1).reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Feature engineering completed\")\n",
    "print(f\"Ready to predict for {len(df_new_latest)} machines\")\n",
    "df_new_latest[['MachineID', 'CumulativeBitRotation', 'Rot_LastHour_Sum']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ea4ec",
   "metadata": {},
   "source": [
    "## Part 10.3: Generate Predictions on New CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for prediction (same features as training)\n",
    "X_new_predict = df_new_latest[feature_cols]\n",
    "\n",
    "# Make predictions using the loaded model\n",
    "predictions_new = loaded_model.predict(X_new_predict)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_new_latest['ML_PredictedDays'] = predictions_new\n",
    "\n",
    "# Calculate risk level\n",
    "df_new_latest['RiskLevel'] = pd.cut(\n",
    "    df_new_latest['ML_PredictedDays'],\n",
    "    bins=[0, 2, 7, 14, 365],\n",
    "    labels=['üî¥ CRITICAL', 'üü† URGENT', 'üü° WARNING', 'üü¢ GOOD']\n",
    ")\n",
    "\n",
    "# Add prediction timestamp\n",
    "df_new_latest['PredictionTimestamp'] = datetime.utcnow()\n",
    "\n",
    "# Calculate rule-based prediction for comparison\n",
    "BIT_LIFETIME = 100000\n",
    "df_new_latest['RemainingRotations'] = BIT_LIFETIME - df_new_latest['CumulativeBitRotation']\n",
    "df_new_latest['RotationsPerHour'] = df_new_latest['Rot_LastHour_Sum']\n",
    "df_new_latest['HoursUntilReplacement'] = df_new_latest['RemainingRotations'] / (df_new_latest['RotationsPerHour'] + 0.1)\n",
    "df_new_latest['DaysUntilReplacement'] = (df_new_latest['HoursUntilReplacement'] / 24).clip(0, 365)\n",
    "\n",
    "print(\"‚úÖ Predictions generated on new CSV data\")\n",
    "print(f\"\\nüìä Risk Distribution:\")\n",
    "print(df_new_latest['RiskLevel'].value_counts())\n",
    "print(f\"\\nüîç Sample Predictions:\")\n",
    "df_new_latest[['MachineID', 'CumulativeBitRotation', 'DaysUntilReplacement', 'ML_PredictedDays', 'RiskLevel']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35253942",
   "metadata": {},
   "source": [
    "## Part 10.4: Save New Predictions to Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e247a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns for new prediction table\n",
    "prediction_cols_new = [\n",
    "    'PredictionTimestamp', 'Timestamp', 'MachineID',\n",
    "    'CumulativeBitRotation', 'RotationCount',\n",
    "    'Rot_LastHour_Sum', 'PassRate_LastHour',\n",
    "    'DaysUntilReplacement',  # Rule-based prediction\n",
    "    'ML_PredictedDays',       # ML prediction\n",
    "    'RiskLevel'\n",
    "]\n",
    "\n",
    "df_new_predictions = df_new_latest[prediction_cols_new].copy()\n",
    "\n",
    "# Convert to Spark and save (append to existing predictions)\n",
    "spark_new_predictions = spark.createDataFrame(df_new_predictions)\n",
    "spark_new_predictions.write.mode(\"append\").saveAsTable(\"MaintenanceML.ml_predictions\")\n",
    "\n",
    "print(\"‚úÖ New predictions saved to: MaintenanceML.ml_predictions\")\n",
    "print(f\"Added {len(df_new_predictions)} new predictions\")\n",
    "print(f\"\\nüìà Comparison: ML vs Rule-Based\")\n",
    "comparison = df_new_predictions[['MachineID', 'DaysUntilReplacement', 'ML_PredictedDays', 'RiskLevel']].head(10)\n",
    "print(comparison.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
